[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Sean Trott\nAssistant Professor, PI\nSean is an Assistant Professor of Psychology at Rutgers University and the director of the Language, Cognition, and Computation Lab. He got his PhD from UC San Diego in Cognitive Science in 2022.\nWebsite\n\n\nInterested in joining the lab? Check out the Join page."
  },
  {
    "objectID": "join.html",
    "href": "join.html",
    "title": "Join the Lab",
    "section": "",
    "text": "Thank you for your interest in joining the lab!"
  },
  {
    "objectID": "join.html#prospective-graduate-students",
    "href": "join.html#prospective-graduate-students",
    "title": "Join the Lab",
    "section": "Prospective Graduate Students",
    "text": "Prospective Graduate Students\nI am recruiting graduate students. If you’re interested in working with me, please apply through the Psychology PhD program at Rutgers University–Newark.\nIf you have questions about the lab or want to discuss fit before applying, you’re welcome to reach out by email."
  },
  {
    "objectID": "join.html#postdocs",
    "href": "join.html#postdocs",
    "title": "Join the Lab",
    "section": "Postdocs",
    "text": "Postdocs\nI don’t currently have funding for postdoctoral positions, but if you’re interested in applying for external funding to work in the lab, I’d be happy to discuss possibilities so we could apply for a grant together."
  },
  {
    "objectID": "join.html#contact",
    "href": "join.html#contact",
    "title": "Join the Lab",
    "section": "Contact",
    "text": "Contact\nSean Trott: sean.trott@rutgers.edu"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "The Language, Cognition, and Computation (LCC) Lab investigates research questions at the intersection of Psychology and AI. We use tools like large language models (LLMs) as “model organisms” to study human cognition; in turn, we apply theories and methods from Psychology to better understand how LLMs work under the hood.\nA (non-exhaustive) sample of ongoing research programs is listed below.\nResearch areas: Mental state reasoning · Lexical ambiguity · Epistemology · Embodiment · Calibration and trust\n\n\nMental state reasoning in humans and language models\nWhat can we learn by comparing the behavior of humans and language models on mental state reasoning tasks?\n\nHumans regularly reason about the belief states of others. But how do we acquire this capacity? One hypothesis is that it emerges in part from exposure to language. We use LLMs trained on the distributional statistics of language to ask whether, when, and why sensitivity to belief states reliably emerges from this training protocol. We also investigate the developmental trajectory and mechanistic underpinnings of this behavior in LLMs to better understand the phenomenon in both LLMs and humans.\nSelected papers:\n\nTrott, S., Jones, C., Chang, T., Michaelov, J., & Bergen, B. (2023). Do Large Language Models know what humans know? Cognitive Science. [Link]\nJones, C. R., Trott, S., & Bergen, B. (2024). Comparing Humans and Large Language Models on EPITOME. Transactions of the Association for Computational Linguistics, 12, 803-819. [Link]\n\n\n\n\nRepresentation and processing of lexical ambiguity\nHow do humans and language models represent the multiple meanings of ambiguous words?\n\nMost words are ambiguous, i.e., they mean different things in different contexts. Ambiguity raises a challenge for comprehension, but it’s also an opportunity for research: it provides a window into the representations and mechanisms underlying the contextualization of meaning. In humans, our work focuses on the format of meaning representations (e.g., are they discrete or continuous?). In language models, we focus on the internal mechanisms responsible for disambiguation (e.g., specific attention heads).\nSelected papers:\n\nRivière, P., & Trott, S. (2026). Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity. Transactions of the Association for Computational Linguistics. [Link] [Code and data]\nRivière, P. D., Beatty-Martínez, A. L., & Trott, S. (2025, April). Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (pp. 8322-8338). [Link] [Code] [HuggingFace dataset]\nTrott, S., & Bergen, B. (2023). Word meaning is both categorical and continuous. Psychological Review, 130(5), 1239. [Link] [Code and data]\nTrott, S., & Bergen, B. (2021, August). RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English). In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 7077-7087). [Link] [Dataset and code][HuggingFace dataset]\n\n\n\n\nEpistemological foundations of “LLM-ology”\nCan we develop a systematic, generalizable science of LLMs?\n\nThe use of LLMs in Cognitive Science research is relatively novel, as is the scientific study of LLMs themselves (“LLM-ology”). This raises a number of epistemological challenges:\n\nChallenges of measurement: does the same test “mean the same thing” when applied to humans and LLMs, or does it exhibit differential construct validity? What is the functional scope of a particular mechanism?\nChallenges of generalizability: what does research on a particular LLM tell us about LLMs more broadly? And in what sense are LLMs “model organisms” for research on human cognition?\nChallenges of ontology: what “kind of thing” is an LLM in the first place and what conceptual frameworks are most useful for understanding these systems?\n\nSelected papers:\n\nTrott, S. Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research. In Mechanistic Interpretability Workshop at NeurIPS 2025. [Link] [Code and data]\nTrott, S. (2024). Large language models and the wisdom of small crowds. Open Mind, 8, 723-738. [Link] [Code and data]\nTrott, S., Jones, C., Chang, T., Michaelov, J., & Bergen, B. (2023). Do Large Language Models know what humans know? Cognitive Science. [Link]\n\n\n\n\nEmbodiment and conceptual knowledge\nHow is conceptual knowledge shaped by sensorimotor and linguistic experience?\n\nHumans have bodies and move through space. What role do these perceptual and motor experiences play in shaping our conceptual representations? And how do these experiences interact with our knowledge of the symbolic, categorical nature of human language? We use neural networks trained on various input modalities (e.g., vision models, language models, and vision-language models) to ask which best accounts for human conceptual knowledge.\nSelected papers:\n\nRivière, P. D., Parkinson-Coombs, O., Jones, C., & Trott, S. (2025, July). Does Language Stabilize Quantity Representations in Vision Transformers? In (Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 47, No. 47)*. [Link]\nJones, C. R., Bergen, B., & Trott, S. (2024). Do multimodal large language models and humans ground language similarly?. Computational Linguistics, 50(4), 1415-1440. [Link]\nTrott, S., & Bergen, B. (2022). Contextualized sensorimotor norms: Multi-dimensional measures of sensorimotor strength for ambiguous English words, in context. [Link]\n\n\n\n\nCalibration and trust\nWhat kinds of mental models do individuals have about the capabilities and limitations of AI tools, and how we can encourage appropriate calibration?\n\nAI tools like LLMs display seemingly remarkable abilities—but also, in some cases, surprising and inexplicable failures. This makes calibrating our expectations of what these systems can and can’t do very difficult. We study potential use cases of LLMs (e.g., in scientific research) as well as the metaphors and mental models individuals form from observing the behavior of LLMs.\nSelected papers:\n\nTrott, S. (2024). Can large language models help augment English psycholinguistic datasets?. Behavior Research Methods, 56(6), 6082-6100. [Link] [Code and data]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language, Cognition, and Computation Lab",
    "section": "",
    "text": "The Language, Cognition, and Computation (LCC) Lab is a research group in the Department of Psychology at Rutgers University–Newark, directed by Sean Trott. We investigate research questions at the intersection of Psychology and AI (see our research page for more)."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Language, Cognition, and Computation Lab",
    "section": "News",
    "text": "News\n\nJanuary 2026: The LCC Lab officially launches at Rutgers University–Newark!"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Language, Cognition, and Computation Lab",
    "section": "Contact",
    "text": "Contact\nSean Trott\nAssistant Professor, Department of Psychology\nRutgers University–Newark\nEmail: sean.trott@rutgers.edu"
  }
]