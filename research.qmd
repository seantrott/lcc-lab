---
title: "Research"
---

The Language, Cognition, and Computation Lab uses large language models as "model organisms" to study human cognition. We combine methods from cognitive science, linguistics, and AI to understand how meaning is represented and processed—in both minds and machines.


::: {.research-section}
::: {.section-header}
### Theory of Mind in Language Models
Do LLMs represent mental states, or just predict text?
:::

We investigate whether and how language models develop theory of mind capabilities through false belief tasks, activation patching experiments, and analysis of attention patterns during social reasoning. Our work examines multiple open-weight models to understand the mechanisms underlying apparent mental state reasoning and whether these systems implement human-like representations or alternative computational strategies.

**Selected papers:**

- Trott, S., Jones, C., Chang, T., Michaelov, J., & Bergen, B. (2023). Do Large Language Models know what humans know? *Cognitive Science.* [[Link]](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13309)
- Jones, C. R., Trott, S., & Bergen, B. (2024). Comparing Humans and Large Language Models on EPITOME. *Transactions of the Association for Computational Linguistics, 12, 803-819.* [[Link]](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00674/122721)

:::

::: {.research-section}
::: {.section-header}
### Lexical Ambiguity and Meaning Representation
How do humans and language models represent the multiple meanings of ambiguous words?
:::

The prevalence of lexical ambiguity raises the question of how human minds (and LLMs) process and represent the meanings of ambiguous words.

Traditionally, words and their meanings as conceived as discrete entries in a mental dictionary. But meaning is often dynamically modulated in different contexts. I’ve been using LLMs to explore an alternative account, in which word meanings are viewed as attractors in a continuous state-space, and then asking whether there is evidence for category boundaries atop this continuous space.

**Selected papers:**

- Rivière, P., & Trott, S. (To appear). Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity. *TACL.*
- Rivière, P. D., Beatty-Martínez, A. L., & Trott, S. (2025, April). Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis. In *Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (pp. 8322-8338)*. [[Link to paper]](https://aclanthology.org/2025.naacl-long.422/) [[Link to GitHub code]](https://github.com/seantrott/spanish_norms) [[Link to HuggingFace dataset]](https://huggingface.co/datasets/seantrott/sawc)
- Trott, S., & Bergen, B. (2023). Word meaning is both categorical and continuous. *Psychological Review.* [[Link]](https://www.researchgate.net/profile/Sean-Trott/publication/369116867_Word_meaning_is_both_categorical_and_continuous/links/656dfb46a760eb7cc748b026/Word-Meaning-Is-Both-Categorical-and-Continuous.pdf)
- Trott, S., & Bergen, B. (2021). RAW-C: Relatedness of Ambiguous Words, in Context (A New Lexical Resource for English). *ACL-IJCNLP-2021.* [[Link to paper]](https://aclanthology.org/2021.acl-long.550/) [[Link to dataset and code]](https://github.com/seantrott/raw-c)[[Link to HuggingFace dataset]](https://huggingface.co/datasets/seantrott/rawc)


:::



Other topics:

- sensorimotor grounding 
- epistemology --> both for LLM-ology and for LLMs as model organisms (distributional baselines, generalizability, construct validity, and epistemic iteration)
- ToM (modify)
- situation models / representations more generally


:::